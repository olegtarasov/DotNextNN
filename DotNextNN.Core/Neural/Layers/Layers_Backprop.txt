Base
==============

public abstract void ClearGradients();

public abstract void Optimize(OptimizerBase optimizer);

/// <summary>
///     Propagates next layer sensitivity to input, accumulating gradients for optimization
/// </summary>
/// <param name="outSens">Sequence of sensitivity matrices of next layer</param>
/// <param name="needInputSens">Calculate input sensitivity for further propagation</param>
/// <param name="clearGrad">Clear gradients before backpropagation.</param>
/// <returns></returns>
public virtual Matrix BackPropagate(Matrix outSens, bool needInputSens = true, bool clearGrad = true)
{
    return outSens;
}

/// <summary>
///     Calculates matched error (out-target) and propagates it through layer to inputs
/// </summary>
public virtual Matrix ErrorPropagate(Matrix target)
{
    if (ErrorFunction == null)
    {
        throw new InvalidOperationException("Layer error function is not specified!");
    }

    return ErrorFunction.BackpropagateError(Output, target);
}


Linear
===================

public override void Optimize(OptimizerBase optimizer)
{
    optimizer.Optimize(_weights);
    optimizer.Optimize(_bias);
}


public override Matrix ErrorPropagate(Matrix target)
{
    return BackPropagate(base.ErrorPropagate(target));
}

public override Matrix BackPropagate(Matrix outSens, bool needInputSens = true, bool clearGrad = true)
{
    if (clearGrad)
    {
        ClearGradients();
    }

    var yIdentity = new Matrix(BatchSize, 1, 1.0f);
    Matrix inputSens = new Matrix(Input.Rows, BatchSize);

    _weights.Gradient.Accumulate(outSens, Input, transposeB: TransposeOptions.Transpose);
    if (BatchSize > 1)
    {
        _bias.Gradient.Accumulate(outSens, yIdentity);
    }
    else
    {
        _bias.Gradient.Accumulate(outSens);
    }

    if (needInputSens)
    {
        inputSens.Accumulate(_weights.Weight, outSens, transposeA: TransposeOptions.Transpose);
    }
            
    return inputSens;
}

public override void ClearGradients()
{
    _weights.ClearGrad();
    _bias.ClearGrad();
}


Sigmoid
==========================================

public override void Optimize(OptimizerBase optimizer)
{
}

public override Matrix ErrorPropagate(Matrix targets)
{
    return BackPropagate(base.ErrorPropagate(targets));
}

public override Matrix BackPropagate(Matrix outSens, bool needInputSens = true, bool clearGrad = true)
{
    var ones = new Matrix(outSens.Rows, outSens.Cols, 1.0f);
    // osens ^ s(x) ^ (1 - s(x))
    var iSens = (ones - Output) ^ Output ^ outSens;
    return iSens;
}

public override void ClearGradients()
{
}


SoftMax
====================================
public override void Optimize(OptimizerBase optimizer)
{
}

public override Matrix BackPropagate(Matrix outSens, bool needInputSens = true, bool clearGrad = true)
{
    throw new NotSupportedException("SoftMax layer should be the last layer!");
}

public override void ClearGradients()
{
}